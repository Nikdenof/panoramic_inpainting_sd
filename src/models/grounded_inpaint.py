import logging
import os

import cv2
import numpy as np
import torch
from PIL import Image
from diffusers import StableDiffusionInpaintPipeline
from tqdm import tqdm

from GSAM.GroundingDINO.groundingdino.util.inference import load_image
from src.models.predict_dino import detect, get_dino_model
from src.models.predict_sam import get_sam_model, segment
from src.utils.constants import SD_SEED, DINO2SD_DICT
from src.features.opencv_transforms import mask_pil_preprocess


def cv2_display(image_og):
    return cv2.cvtColor(np.array(image_og), cv2.COLOR_BGR2RGB)


# def dilate_mask(mask_in):
#     # Convert the PIL image (mode '1') to a NumPy array
#     mask_array = np.array(mask_in)
#
#     # OpenCV's dilation expects the input array to be of type uint8
#     mask_array = mask_array.astype(np.uint8) * 255  # Convert to 0 and 255
#
#     # Define the structuring element for dilation and apply dilation
#     kernel = np.ones((8, 8), np.uint8)  # Adjust the kernel size as needed
#     dilated_mask_array = cv2.dilate(mask_array, kernel, iterations=1)
#
#     dilated_in_mask = Image.fromarray(dilated_mask_array).convert("1")
#     return dilated_in_mask


def draw_mask(mask, image, random_color=True):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)
    else:
        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)

    annotated_frame_pil = Image.fromarray(image).convert("RGBA")
    mask_image_pil = Image.fromarray(
        (mask_image.cpu().numpy() * 255).astype(np.uint8)
    ).convert("RGBA")

    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))


def generate_image(
    in_image,
    in_mask,
    prompt,
    negative_prompt,
    pipe,
    seed,
    device,
    resize_hw=512,
    dilate_bool=False,
    strength_param=0.8,
    guidance_param=10,
):
    # resize for inpainting
    w, h = in_image.size
    if resize_hw is not None:
        in_image = in_image.resize((resize_hw, resize_hw))
        in_mask = in_mask.resize((resize_hw, resize_hw))


    generator = torch.Generator(device).manual_seed(seed)

    result = pipe(
        image=in_image,
        mask_image=in_mask,
        prompt=prompt,
        negative_prompt=negative_prompt,
        generator=generator,
        strength=strength_param,  # 1 is default
        guidance_scale=guidance_param,  # 7.5 is default
        padding_mask_crop=32,
    )
    result = result.images[0]

    return result.resize((w, h))


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    groundingdino_model = get_dino_model(device)
    sam_predictor = get_sam_model(device)
    sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-inpainting",
        torch_dtype=torch.float16,
    ).to(device)
    # disable the progress bar
    sd_pipe.set_progress_bar_config(disable=True)

    # image_directory = "../../data/raw/vadim_data_v0/"
    image_directory = "../../data/raw/vadim_data_v1/Archive/"
    out_directory = "../../data/processed/sky_sd_test_v3/"
    os.makedirs(out_directory, exist_ok=True)

    for img_name in tqdm(os.listdir(image_directory), desc="Processing images"):
        local_image_path = os.path.join(image_directory, img_name)
        # local_image_path = "../../data/raw/vadim_data_v0/1685684068_klau-club-p-dom-s-gazonom-2.jpeg"
        output_image_path = os.path.join(out_directory, img_name)

        image_source, image = load_image(local_image_path)

        out_image = inpaint_image(
            image_source,
            image,
            groundingdino_model,
            sam_predictor,
            sd_pipe,
            device,
            DINO2SD_DICT,
        )
        out_image_np = np.array(out_image, dtype=np.uint8)
        stacked_image = cv2.vconcat([image_source, out_image_np])
        img_rgb = cv2.cvtColor(stacked_image, cv2.COLOR_BGR2RGB)
        cv2.imwrite(output_image_path, img_rgb)

    logging.info("End of main function")


def inpaint_image(
    image_source,
    image,
    groundingdino_model,
    sam_predictor,
    sd_pipe,
    device,
    prompt_dict,
):
    classes2detect = list(prompt_dict.keys())
    dino_prompt_in = f"{'. '.join(classes2detect)}."
    annotated_frame, detected_boxes, detected_classes = detect(
        image, image_source, text_prompt=dino_prompt_in, model=groundingdino_model
    )

    # Step 2. Segmenting using SAM based on bbox generated by DINO
    segmented_frame_masks = segment(
        image_source, sam_predictor, boxes=detected_boxes, device=device
    )
    annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)

    # Step 3. Inpainting process
    image_pil = Image.fromarray(image_source)
    for mask_tensor, class_detected in zip(segmented_frame_masks, detected_classes):
        mask_og = mask_tensor[0].cpu().numpy()

        image_mask_pil = Image.fromarray(mask_og)
        image_mask_pil = mask_pil_preprocess(image_mask_pil)

        prompt_list = prompt_dict.get(class_detected)
        if prompt_list is None:
            continue

        fill_color = prompt_list[2]
        img_interim = np.array(image_pil)

        # Filling by initial mask
        img_interim[mask_og] = fill_color
        image_pil = Image.fromarray(img_interim)

        image_pil = generate_image(
            in_image=image_pil,
            in_mask=image_mask_pil,
            prompt=prompt_list[0],
            negative_prompt=prompt_list[1],
            pipe=sd_pipe,
            seed=SD_SEED,
            device=device,
            strength_param=0.8,
            guidance_param=10,
        )
        logging.info(f"Iteration {class_detected} finished")

    return image_pil


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )
    main()
