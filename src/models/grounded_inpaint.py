import logging

import cv2
import numpy as np
import torch
from PIL import Image
from diffusers import StableDiffusionInpaintPipeline

from GSAM.GroundingDINO.groundingdino.util.inference import (
    load_image,
)
from src.models.predict_dino import detect, get_dino_model
from src.utils.constants import MODEL_URL, MODEL_PATH, SD_SEED
from src.models.predict_sam import get_sam_model, segment


def cv2_display(image_og):
    return cv2.cvtColor(np.array(image_og), cv2.COLOR_BGR2RGB)


def dilate_mask(mask_in):
    # Convert the PIL image (mode '1') to a NumPy array
    mask_array = np.array(mask_in)

    # OpenCV's dilation expects the input array to be of type uint8
    mask_array = mask_array.astype(np.uint8) * 255  # Convert to 0 and 255

    # Define the structuring element for dilation and apply dilation
    kernel = np.ones((8, 8), np.uint8)  # Adjust the kernel size as needed
    dilated_mask_array = cv2.dilate(mask_array, kernel, iterations=1)

    dilated_in_mask = Image.fromarray(dilated_mask_array).convert("1")
    return dilated_in_mask


def draw_mask(mask, image, random_color=True):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)
    else:
        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)

    annotated_frame_pil = Image.fromarray(image).convert("RGBA")
    mask_image_pil = Image.fromarray(
        (mask_image.cpu().numpy() * 255).astype(np.uint8)
    ).convert("RGBA")

    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))


def generate_image(
    in_image,
    in_mask,
    prompt,
    negative_prompt,
    pipe,
    seed,
    device,
    resize_hw=512,
    dilate_bool=False,
    strength_param=0.8,
    guidance_param=10,
):
    # resize for inpainting
    w, h = in_image.size
    if resize_hw is not None:
        in_image = in_image.resize((resize_hw, resize_hw))
        in_mask = in_mask.resize((resize_hw, resize_hw))

    if dilate_bool:
        in_mask = dilate_mask(in_mask)

    generator = torch.Generator(device).manual_seed(seed)

    result = pipe(
        image=in_image,
        mask_image=in_mask,
        prompt=prompt,
        negative_prompt=negative_prompt,
        generator=generator,
        strength=strength_param,  # 1 is default
        guidance_scale=guidance_param,  # 7.5 is default
        padding_mask_crop=32,
    )
    result = result.images[0]

    return result.resize((w, h))


def main():
    # local_image_path = "../../GSAM/assets/inpaint_demo.jpg"
    # local_image_path = "../../data/raw/test_images/sky.webp"
    local_image_path = "../../data/raw/test_images/grass.jpg"
    image_source, image = load_image(local_image_path)
    out_image = inpaint_image(image_source, image)

    logging.info("End of main function")


def inpaint_image(image_source, image):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    groundingdino_model = get_dino_model(device)
    sam_predictor = get_sam_model(device)

    sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-inpainting",
        torch_dtype=torch.float16,
    ).to(device)

    dino_prompt = "grass"
    annotated_frame, detected_boxes = detect(
        image, image_source, text_prompt=dino_prompt, model=groundingdino_model
    )

    # Step 2. Segmenting using SAM based on bbox generated by DINO
    segmented_frame_masks = segment(
        image_source, sam_predictor, boxes=detected_boxes, device=device
    )
    annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)

    # Step 3. Inpainting process
    mask = segmented_frame_masks[0][0].cpu().numpy()
    image_source_pil = Image.fromarray(image_source)
    image_mask_pil = Image.fromarray(mask)

    prompt = "green_grass, realistic, summer_grass, high_resolution, lawn"
    negative_prompt = (
        "low_resolution, ugly, unrealistic, branches, gold_leaves, structural_extension, trees, debris, sprigs, "
        "black_twigs, blank_spots, grass_bare_patches, mud, ground"
    )

    generated_image = generate_image(
        in_image=image_source_pil,
        in_mask=image_mask_pil,
        prompt=prompt,
        negative_prompt=negative_prompt,
        pipe=sd_pipe,
        seed=SD_SEED,
        device=device,
        strength_param=0.8,
        guidance_param=10,
    )
    generated_image_debug = cv2_display(generated_image)
    return generated_image


if __name__ == "__main__":
    main()
